# -*- coding: utf-8 -*-
"""startup_mlmodel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tkskBC4WvsapSk33W07iMACh4pNJYJwz
"""

import pandas as pd

df = pd.read_csv('realistic_startups_dataset.csv')

print(df.head())

print(df.info())

print(df.isnull().sum())

df.describe()

import numpy as np

from sklearn.preprocessing import LabelEncoder, StandardScaler

from scipy.stats import zscore



categorical_columns = ["Industry", "Market Size"]

numerical_columns = ["Total Funding (M$)", "Number of Investors", "Growth Rate (%)",
                     "Revenue (M$)", "Years Since Founded", "Investor Reputation"]

df.fillna(df.median(numeric_only=True), inplace=True)

market_size_mapping = {"Small": 1, "Medium": 2, "Large": 3}

df["Market Size"] = df["Market Size"].map(market_size_mapping)

df["Funding per Investor"] = df["Total Funding (M$)"] / (df["Number of Investors"] + 1)

df["Revenue to Funding Ratio"] = df["Revenue (M$)"] / (df["Total Funding (M$)"] + 1)

def remove_outliers(df, columns, threshold=3):

    for col in columns:

        df = df[np.abs(zscore(df[col])) < threshold]

    return df

df = remove_outliers(df, numerical_columns)

corr_matrix = df[numerical_columns].corr().abs()

upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))

to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.85)]

df.drop(columns=to_drop, inplace=True)

if "Investor Reputation" in df.columns:

    le = LabelEncoder()

    df["Investor Reputation"] = le.fit_transform(df["Investor Reputation"])

df = pd.get_dummies(df, columns=["Industry"], drop_first=True)

scaler = StandardScaler()

df[numerical_columns] = scaler.fit_transform(df[numerical_columns])

from google.colab import files

output_path = "preprocessed_startup_dataset.csv"

df.to_csv(output_path, index=False)

files.download(output_path)

import xgboost as xgb

import joblib

import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score

from sklearn.preprocessing import StandardScaler

from sklearn.metrics import mean_squared_error, r2_score

df = pd.read_csv('preprocessed_startup_dataset.csv')

if "Startup Name" in df.columns:

    df.drop(columns=["Startup Name"], inplace=True)

df = df.apply(pd.to_numeric, errors="coerce")

df.fillna(df.mean(numeric_only=True), inplace=True)

target_column = "Revenue (M$)"

X = df.drop(columns=[target_column])

y = df[target_column]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()

X_train = scaler.fit_transform(X_train)

X_test = scaler.transform(X_test)

xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)

param_grid = {
    "n_estimators": [100, 300, 500],
    "learning_rate": [0.01, 0.1, 0.2],
    "max_depth": [3, 5, 7],
    "subsample": [0.7, 0.8, 0.9],
    "colsample_bytree": [0.7, 0.8, 0.9]
}

grid_search = GridSearchCV(xgb_model, param_grid, cv=5, scoring="r2", n_jobs=-1, verbose=1)

grid_search.fit(X_train, y_train)

tuned_model = grid_search.best_estimator_

cv_scores = cross_val_score(tuned_model, X_train, y_train, cv=5, scoring="r2")

print("Cross-Validation R² Scores:", cv_scores)

print("Mean CV R² Score:", np.mean(cv_scores))

y_pred = tuned_model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)

r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse:.4f}")

print(f"R-squared Score: {r2:.4f}")

joblib.dump(tuned_model, "xgboost_startup_model_v3.pkl")

print("Model saved as xgboost_startup_model_v3.pkl")

plt.figure(figsize=(10, 5))

xgb.plot_importance(tuned_model)

plt.show()

