# -*- coding: utf-8 -*-
"""Startup_dlmodel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15n0_j0Ux89U-g0DIehsnPo_al7SRnQD7
"""

import pandas as pd

# Load the dataset (Replace 'your_dataset.csv' with the actual filename)
df = pd.read_csv('Time_series_data.csv')

# Display first few rows
print(df.head())

# Check dataset info
print(df.info())

# Check for missing values
print(df.isnull().sum())

# import pandas as pd
# import numpy as np
# from sklearn.preprocessing import StandardScaler



# # Convert date columns to datetime format
# if "Founded Date" in df.columns and "Funding Date" in df.columns:
#     df["Founded Date"] = pd.to_datetime(df["Founded Date"], errors='coerce')
#     df["Funding Date"] = pd.to_datetime(df["Funding Date"], errors='coerce')

#     # Extract useful time-based features
#     df["Years Since Founded"] = (df["Funding Date"].dt.year - df["Founded Date"].dt.year).fillna(0)
#     df["Funding Month"] = df["Funding Date"].dt.month
#     df["Funding Year"] = df["Funding Date"].dt.year
# else:
#     print("Warning: Date columns missing!")

# # Drop less relevant columns (if they exist)
# drop_cols = [ "Headquarters", "Founded Date", "Funding Date"]
# df.drop(columns=[col for col in drop_cols if col in df.columns], inplace=True)

# # Encode categorical variables (only if they exist)
# categorical_cols = ["Industry", "Market Trend"]
# df = pd.get_dummies(df, columns=[col for col in categorical_cols if col in df.columns], drop_first=True)

# # Select only available numerical columns
# numerical_cols = ["Employee Count", "Funding Amount (M$)", "Number of Investors", "Investor Reputation",
#                   "Economic Indicator", "Sector Growth Rate (%)", "Estimated Valuation (M$)", "Revenue (M$)",
#                   "Profit Margin (%)", "Years Since Founded"]

# available_numerical_cols = [col for col in numerical_cols if col in df.columns]

# # Scale numerical features (only if they exist)
# if available_numerical_cols:
#     scaler = StandardScaler()
#     df[available_numerical_cols] = scaler.fit_transform(df[available_numerical_cols])
# else:
#     print("Warning: No numerical columns available for scaling!")

# # Fill any remaining missing values
# df.fillna(0, inplace=True)

# # Save preprocessed dataset
# preprocessed_file_path = "Preprocessed_time_series_data.csv"
# df.to_csv(preprocessed_file_path, index=False)

# print("Preprocessing complete! Saved as Preprocessed_time_series_data.csv")

# import pandas as pd
# import numpy as np
# import tensorflow as tf
# from tensorflow.keras.models import Sequential
# from tensorflow.keras.layers import LSTM, Dense, Dropout
# from sklearn.preprocessing import MinMaxScaler
# from sklearn.model_selection import train_test_split
# import matplotlib.pyplot as plt

# # Load preprocessed dataset
# df = pd.read_csv('Preprocessed_time_series_data.csv')

# # Sort by funding date to maintain sequence
# df = df.sort_values(by=["Startup Name", "Funding Year", "Funding Month"])

# # Select relevant columns
# features = ["Funding Amount (M$)", "Number of Investors", "Investor Reputation",
#             "Economic Indicator", "Sector Growth Rate (%)", "Estimated Valuation (M$)"]
# target = "Funding Amount (M$)"

# # Normalize features
# scaler = MinMaxScaler()
# df[features] = scaler.fit_transform(df[features])

# # Prepare data for LSTM (Sliding Window Approach)
# def create_sequences(data, target_col, seq_length=6):
#     X, y = [], []
#     for i in range(len(data) - seq_length):
#         X.append(data[i:i + seq_length])
#         y.append(data[i + seq_length, target_col])
#     return np.array(X), np.array(y)

# # Convert dataframe to numpy array
# data = df[features].values

# # Define sequence length (using past 6 months to predict next month)
# seq_length = 6
# X, y = create_sequences(data, target_col=0, seq_length=seq_length)

# # Train-test split
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)

# # Build LSTM Model
# model = Sequential([
#     LSTM(64, return_sequences=True, input_shape=(seq_length, len(features))),
#     Dropout(0.2),
#     LSTM(32, return_sequences=False),
#     Dropout(0.2),
#     Dense(16, activation='relu'),
#     Dense(1)  # Predicting funding amount
# ])

# # Compile model
# model.compile(optimizer='adam', loss='mse')

# # Train model
# history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), verbose=1)

# # Evaluate model
# loss = model.evaluate(X_test, y_test)
# print(f"Test Loss (MSE): {loss:.4f}")

# # Predictions
# y_pred = model.predict(X_test)

# # Plot actual vs predicted funding trends
# plt.figure(figsize=(10, 5))
# plt.plot(y_test, label='Actual Funding')
# plt.plot(y_pred, label='Predicted Funding', linestyle='dashed')
# plt.legend()
# plt.title("Actual vs Predicted Funding Amounts")
# plt.show()

# # Save model
# model.save("/mnt/data/lstm_funding_model.h5")
# print("Model saved as lstm_funding_model.h5")

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.layers import LSTM, Dense, Dropout
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# Load dataset
df = pd.read_csv("Time_series_data.csv")

# Convert date columns to datetime format
if "Founded Date" in df.columns and "Funding Date" in df.columns:
    df["Founded Date"] = pd.to_datetime(df["Founded Date"], errors='coerce')
    df["Funding Date"] = pd.to_datetime(df["Funding Date"], errors='coerce')

    # Extract useful time-based features
    df["Years Since Founded"] = (df["Funding Date"].dt.year - df["Founded Date"].dt.year).fillna(0)
    df["Funding Month"] = df["Funding Date"].dt.month
    df["Funding Year"] = df["Funding Date"].dt.year
else:
    print("Warning: Date columns missing!")

# Keep "Startup Name" until sorting, then drop it
drop_cols = ["Headquarters", "Founded Date", "Funding Date"]  # Keep "Startup Name" initially
df.drop(columns=[col for col in drop_cols if col in df.columns], inplace=True)

# Sort by Startup Name & Funding Date
if "Startup Name" in df.columns:
    df = df.sort_values(by=["Startup Name", "Funding Year", "Funding Month"])
    df.drop(columns=["Startup Name"], inplace=True)  # Drop after sorting
else:
    print("Warning: 'Startup Name' column missing! Sorting by Funding Year & Month.")
    df = df.sort_values(by=["Funding Year", "Funding Month"])

# Encode categorical variables (only if they exist)
categorical_cols = ["Industry", "Market Trend"]
df = pd.get_dummies(df, columns=[col for col in categorical_cols if col in df.columns], drop_first=True)

# Select only available numerical columns
numerical_cols = ["Employee Count", "Funding Amount (M$)", "Number of Investors", "Investor Reputation",
                  "Economic Indicator", "Sector Growth Rate (%)", "Estimated Valuation (M$)", "Revenue (M$)",
                  "Profit Margin (%)", "Years Since Founded"]

available_numerical_cols = [col for col in numerical_cols if col in df.columns]

# Scale numerical features (only if they exist)
if available_numerical_cols:
    scaler = StandardScaler()
    df[available_numerical_cols] = scaler.fit_transform(df[available_numerical_cols])
else:
    print("Warning: No numerical columns available for scaling!")

# Fill any remaining missing values
df.fillna(0, inplace=True)

# Save preprocessed dataset
preprocessed_file_path = "Preprocessed_time_series_data.csv"
df.to_csv(preprocessed_file_path, index=False)

print("Preprocessing complete! Saved as Preprocessed_time_series_data.csv")

# Load preprocessed dataset
df = pd.read_csv(preprocessed_file_path)

# Select relevant columns
features = ["Funding Amount (M$)", "Number of Investors", "Investor Reputation",
            "Economic Indicator", "Sector Growth Rate (%)", "Estimated Valuation (M$)"]
target = "Funding Amount (M$)"

# Normalize features
scaler = MinMaxScaler()
df[features] = scaler.fit_transform(df[features])

# Prepare data for LSTM (Sliding Window Approach)
def create_sequences(data, target_col, seq_length=6):
    X, y = [], []
    for i in range(len(data) - seq_length):
        X.append(data[i:i + seq_length])
        y.append(data[i + seq_length, target_col])
    return np.array(X), np.array(y)

# Convert dataframe to numpy array
data = df[features].values

# Define sequence length (using past 6 months to predict next month)
seq_length = 12
X, y = create_sequences(data, target_col=0, seq_length=seq_length)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)

# Build LSTM Model
model = Sequential([
    LSTM(128, return_sequences=True, input_shape=(seq_length, len(features))),
    Dropout(0.3),
    LSTM(64, return_sequences=False),
    Dropout(0.3),
    Dense(32, activation='relu'),
    Dense(1)
])


# Compile model
model.compile(optimizer='adam', loss='mse')


early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

history = model.fit(X_train, y_train, epochs=100, batch_size=32,
                    validation_data=(X_test, y_test), verbose=1,
                    callbacks=[early_stopping])

# Train model
history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), verbose=1)

# Evaluate model
loss = model.evaluate(X_test, y_test)
print(f"Test Loss (MSE): {loss:.4f}")

# Predictions
y_pred = model.predict(X_test)

# Plot actual vs predicted funding trends
plt.figure(figsize=(10, 5))
plt.plot(y_test, label='Actual Funding')
plt.plot(y_pred, label='Predicted Funding', linestyle='dashed')
plt.legend()
plt.title("Actual vs Predicted Funding Amounts")
plt.show()

# Save model
model.save("lstm_funding_model.keras")
print("Model saved as lstm_funding_model.keras")

num_samples = len(df)
print(f"Number of samples: {num_samples}")